#tokenized.corpus <- txt.to.words.ext(raw.corpus, language = "English.all",
#                                     splitting.rule = "[!(;'?\n^).,>\":= \u2014\u2013]+")
glenarvon = fromJSON(file = "./tokenized_texts/lam_glenarvon.json")
thelastman = fromJSON(file = "./tokenized_texts/mws_the-last-man.json")
stirvyne = fromJSON(file = "./tokenized_texts/pbs_st-irvyne.json")
zastrozzi = fromJSON(file = "./tokenized_texts/pbs_zastrozzi.json")
frankenstein = fromJSON(file = "/Users/tim/GitHub/frankenstein-v2/sga-data/output/text_list_processed_ands.json")
tokenized.corpus = list(glenarvon, thelastman, stirvyne, zastrozzi, frankenstein)
names(tokenized.corpus) = c("lam_glenarvon", "mws_the-last-man", "pbs_st-irvyne", "pbs_zastrozzi", "???_frankenstein")
summary(tokenized.corpus)
sample_size = 400
sample_shift = 100
sample_overlap = sample_size - sample_shift
sliced.corpus <- make.samples(tokenized.corpus, sampling = "normal.sampling",
sample.overlap = sample_overlap,
sample.size = sample_size)
# Temporary list of frequent function words, eventually needs to based on Frankenstein as well
frequent.features <- fromJSON(file = "./f_words.json")
freqs <- make.table.of.frequencies(sliced.corpus, features = frequent.features)
#pca.results = stylo(frequencies = freqs, analysis.type = "PCR",
#      custom.graph.title = "Lamb vs. the Shelleys",
#      mfw.min = 200, mfw.max = 200,
#      pca.visual.flavour = "loadings",
#      write.png.file = FALSE, gui = FALSE)
#summary(pca.results)
#par(mfrow = c(1, 1))
# check how pca.coordinates are calculated from loadings and frequencies
#freq_table = pca.results$table.with.all.freqs
#load_table = t(pca.results$pca.rotation)
#coor_table = pca.results$pca.coordinates
#sum(freq_table["PBS_zastrozzi.txt_4",] * load_table["PC1",])
# fuck it let's do our own PCA
library(ggbiplot)
samples = as.data.frame(as.matrix.data.frame(freqs))
rnames = rownames(freqs)
cnames = colnames(freqs)
rownames(samples) = rnames
colnames(samples) = cnames
num_franken_samples = (length(frankenstein) - sample_overlap) %/% sample_shift
num_lam_samples = (length(glenarvon) - sample_overlap) %/% sample_shift
num_mws_samples = (length(thelastman) - sample_overlap) %/% sample_shift
num_pbs_samples = (length(stirvyne) - sample_overlap) %/% sample_shift + (length(zastrozzi) - sample_overlap) %/% sample_shift
training_samples = samples[1:(NROW(samples) - num_franken_samples),]
franken_samples = samples[(NROW(training_samples) + 1):NROW(samples),]
authors = c(rep("Lamb", num_lam_samples),
rep("Mary Shelley", num_mws_samples),
rep("Percy Shelley", num_pbs_samples))
training_samples.pca = prcomp(training_samples, center = TRUE, scale. = TRUE)
ggbiplot(training_samples.pca, labels = rownames(training_samples),
groups = authors, var.axes = TRUE, var.scale = 0.2, varname.adjust = 10)
frankenstein_sc = scale(franken_samples, center = training_samples.pca$center)
frankenstein_pred = frankenstein_sc %*% training_samples.pca$rotation
training_plusproj.pca = training_samples.pca
training_plusproj.pca$x = rbind(training_plusproj.pca$x, frankenstein_pred)
ggbiplot(training_plusproj.pca,
#         labels = rownames(rbind(training_samples, franken_samples)),
groups = c(authors, rep("Unknown", num_franken_samples)),
var.axes = TRUE, var.scale = 0.2, varname.adjust = 10)
# Add analysis in which PCs are trained on just the Shelley's
frequent.features <- fromJSON(file = "./f_words_shelleys.json")
freqs <- make.table.of.frequencies(sliced.corpus, features = frequent.features)
shelley_samples = training_samples[(num_lam_samples + 1):NROW(training_samples),]
shelley_samples.pca = prcomp(shelley_samples, center = TRUE, scale. = TRUE)
frankenstein_sc = scale(franken_samples, center = shelley_samples.pca$center)
frankenstein_pred = frankenstein_sc %*% shelley_samples.pca$rotation
shelley_plusproj.pca = shelley_samples.pca
shelley_plusproj.pca$x = rbind(shelley_plusproj.pca$x, frankenstein_pred)
ggbiplot(shelley_plusproj.pca,
#         labels = rownames(rbind(shelley_samples, franken_samples)),
groups = c(authors[(num_lam_samples + 1):NROW(training_samples)], rep("Unknown", num_franken_samples)),
var.axes = TRUE, var.scale = 0.5, varname.adjust = 10)
ggbiplot(shelley_plusproj.pca,
labels = rownames(rbind(shelley_samples, franken_samples)),
groups = c(authors[(num_lam_samples + 1):NROW(training_samples)], rep("Unknown", num_franken_samples)),
var.axes = TRUE, var.scale = 0.5, varname.adjust = 10)
library(stylo)
library(rjson)
setwd("/Users/tim/GitHub/frankenstein-v2/analysis")
#raw.corpus <- load.corpus(files = "all", corpus.dir = "./pca_texts",
#                          encoding = "UTF-8")
#tokenized.corpus <- txt.to.words.ext(raw.corpus, language = "English.all",
#                                     splitting.rule = "[!(;'?\n^).,>\":= \u2014\u2013]+")
glenarvon = fromJSON(file = "./tokenized_texts/lam_glenarvon.json")
thelastman = fromJSON(file = "./tokenized_texts/mws_the-last-man.json")
stirvyne = fromJSON(file = "./tokenized_texts/pbs_st-irvyne.json")
zastrozzi = fromJSON(file = "./tokenized_texts/pbs_zastrozzi.json")
frankenstein = fromJSON(file = "/Users/tim/GitHub/frankenstein-v2/sga-data/output/text_list_processed_ands.json")
tokenized.corpus = list(glenarvon, thelastman, stirvyne, zastrozzi, frankenstein)
names(tokenized.corpus) = c("lam_glenarvon", "mws_the-last-man", "pbs_st-irvyne", "pbs_zastrozzi", "???_frankenstein")
summary(tokenized.corpus)
sample_size = 800
sample_shift = 100
sample_overlap = sample_size - sample_shift
sliced.corpus <- make.samples(tokenized.corpus, sampling = "normal.sampling",
sample.overlap = sample_overlap,
sample.size = sample_size)
# Temporary list of frequent function words, eventually needs to based on Frankenstein as well
frequent.features <- fromJSON(file = "./f_words.json")
freqs <- make.table.of.frequencies(sliced.corpus, features = frequent.features)
#pca.results = stylo(frequencies = freqs, analysis.type = "PCR",
#      custom.graph.title = "Lamb vs. the Shelleys",
#      mfw.min = 200, mfw.max = 200,
#      pca.visual.flavour = "loadings",
#      write.png.file = FALSE, gui = FALSE)
#summary(pca.results)
#par(mfrow = c(1, 1))
# check how pca.coordinates are calculated from loadings and frequencies
#freq_table = pca.results$table.with.all.freqs
#load_table = t(pca.results$pca.rotation)
#coor_table = pca.results$pca.coordinates
#sum(freq_table["PBS_zastrozzi.txt_4",] * load_table["PC1",])
# fuck it let's do our own PCA
library(ggbiplot)
samples = as.data.frame(as.matrix.data.frame(freqs))
rnames = rownames(freqs)
cnames = colnames(freqs)
rownames(samples) = rnames
colnames(samples) = cnames
num_franken_samples = (length(frankenstein) - sample_overlap) %/% sample_shift
num_lam_samples = (length(glenarvon) - sample_overlap) %/% sample_shift
num_mws_samples = (length(thelastman) - sample_overlap) %/% sample_shift
num_pbs_samples = (length(stirvyne) - sample_overlap) %/% sample_shift + (length(zastrozzi) - sample_overlap) %/% sample_shift
training_samples = samples[1:(NROW(samples) - num_franken_samples),]
franken_samples = samples[(NROW(training_samples) + 1):NROW(samples),]
authors = c(rep("Lamb", num_lam_samples),
rep("Mary Shelley", num_mws_samples),
rep("Percy Shelley", num_pbs_samples))
training_samples.pca = prcomp(training_samples, center = TRUE, scale. = TRUE)
ggbiplot(training_samples.pca, labels = rownames(training_samples),
groups = authors, var.axes = TRUE, var.scale = 0.2, varname.adjust = 10)
frankenstein_sc = scale(franken_samples, center = training_samples.pca$center)
frankenstein_pred = frankenstein_sc %*% training_samples.pca$rotation
training_plusproj.pca = training_samples.pca
training_plusproj.pca$x = rbind(training_plusproj.pca$x, frankenstein_pred)
ggbiplot(training_plusproj.pca,
#         labels = rownames(rbind(training_samples, franken_samples)),
groups = c(authors, rep("Unknown", num_franken_samples)),
var.axes = TRUE, var.scale = 0.2, varname.adjust = 10)
# Add analysis in which PCs are trained on just the Shelley's
frequent.features <- fromJSON(file = "./f_words_shelleys.json")
freqs <- make.table.of.frequencies(sliced.corpus, features = frequent.features)
shelley_samples = training_samples[(num_lam_samples + 1):NROW(training_samples),]
shelley_samples.pca = prcomp(shelley_samples, center = TRUE, scale. = TRUE)
frankenstein_sc = scale(franken_samples, center = shelley_samples.pca$center)
frankenstein_pred = frankenstein_sc %*% shelley_samples.pca$rotation
shelley_plusproj.pca = shelley_samples.pca
shelley_plusproj.pca$x = rbind(shelley_plusproj.pca$x, frankenstein_pred)
ggbiplot(shelley_plusproj.pca,
#         labels = rownames(rbind(shelley_samples, franken_samples)),
groups = c(authors[(num_lam_samples + 1):NROW(training_samples)], rep("Unknown", num_franken_samples)),
var.axes = TRUE, var.scale = 0.5, varname.adjust = 10)
ggbiplot(shelley_plusproj.pca,
#         labels = rownames(rbind(shelley_samples, franken_samples)),
choices = c(3,4),
groups = c(authors[(num_lam_samples + 1):NROW(training_samples)], rep("Unknown", num_franken_samples)),
var.axes = TRUE, var.scale = 0.5, varname.adjust = 10)
ggbiplot(shelley_plusproj.pca,
#         labels = rownames(rbind(shelley_samples, franken_samples)),
choices = c(2,3),
groups = c(authors[(num_lam_samples + 1):NROW(training_samples)], rep("Unknown", num_franken_samples)),
var.axes = TRUE, var.scale = 0.5, varname.adjust = 10)
ggbiplot(shelley_plusproj.pca,
#         labels = rownames(rbind(shelley_samples, franken_samples)),
choices = c(1,3),
groups = c(authors[(num_lam_samples + 1):NROW(training_samples)], rep("Unknown", num_franken_samples)),
var.axes = TRUE, var.scale = 0.5, varname.adjust = 10)
ggbiplot(shelley_plusproj.pca,
labels = rownames(rbind(shelley_samples, franken_samples)),
choices = c(1,3),
groups = c(authors[(num_lam_samples + 1):NROW(training_samples)], rep("Unknown", num_franken_samples)),
var.axes = TRUE, var.scale = 0.5, varname.adjust = 10)
num_franken_samples
ggbiplot(shelley_plusproj.pca,
#         labels = rownames(rbind(shelley_samples, franken_samples)),
choices = c(1,3),
groups = c(authors[(num_lam_samples + 1):NROW(training_samples)], rep("Unknown", num_franken_samples)),
var.axes = TRUE, var.scale = 0.5, varname.adjust = 10)
library(stylo)
library(rjson)
library(zoo)
setwd("/Users/tim/GitHub/frankenstein-v2/analysis")
# tokenized.test.corpus = load.corpus.and.parse(corpus.dir = "./test_set",
#                                              encoding = "UTF-8",
#                                              splitting.rule = "[!(;'?\n^).,>\":= \u2014\u2013]+")
#summary(tokenized.test.corpus)
# write(toJSON(tokenized.test.corpus$S_text.txt), "frankenstein_tokenized_r.json")
text_tokens = fromJSON(file = "/Users/tim/GitHub/frankenstein-v2/sga-data/output/text_list_processed_ands.json")
tokenized.test.corpus = list(text_tokens)
names(tokenized.test.corpus) = c("S_text")
length(tokenized.test.corpus$S_text) == length(text_tokens)
# tokenized.training.corpus = load.corpus.and.parse(corpus.dir = "./reference_set",
#                                                  encoding = "UTF-8")
thelastman = fromJSON(file = "./tokenized_texts/mws_the-last-man.json")
stirvyne = fromJSON(file = "./tokenized_texts/pbs_st-irvyne.json")
zastrozzi = fromJSON(file = "./tokenized_texts/pbs_zastrozzi.json")
tokenized.training.corpus = list(thelastman, stirvyne, zastrozzi)
names(tokenized.training.corpus) = c("mws_the-last-man", "pbs_st-irvyne", "pbs_zastrozzi")
summary(tokenized.training.corpus)
sample_size = 800
sample_overlap = 700
resolution = sample_size - sample_overlap
sliced.test.corpus = make.samples(tokenized.test.corpus, sampling = "normal.sampling",
sample.size = sample_size, sample.overlap = sample_overlap)
sliced.training.corpus = make.samples(tokenized.training.corpus, sampling = "normal.sampling",
sample.size = sample_size, sample.overlap = 0)
function_words = fromJSON(file = "./f_words_shelleys.json")
results = rolling.classify(test.corpus = sliced.test.corpus,
training.corpus = sliced.training.corpus,
write.png.file = FALSE,
classification.method = "svm", features = function_words,
milestone.points = c(60214), milestone.labels = c("Fair Copy"),
slice.size = sample_size, slice.overlap = sample_overlap)
svm_classification = as.vector(results$classification.results)
hand_tokens = fromJSON(file = "/Users/tim/GitHub/frankenstein-v2/sga-data/output/hand_list_processed.json")
num_samples = as.integer((length(hand_tokens) - sample_overlap) / resolution)
num_words = (num_samples - 1) * resolution + sample_size
culled_hand_tokens = hand_tokens[1:num_words]
hand_matrix = rollapply(culled_hand_tokens, sample_size, by = resolution, c)
hand_groups = split(hand_matrix, row(hand_matrix))
hand_majorities = sapply(hand_groups, function(x) names(which.max(table(x))))
pbs_proportions = sapply(hand_groups, function(x) sum(x == "pbs") / sample_size)
norm_score = 0.5 + results$classification.scores[, 1] / (2 * max(results$classification.scores))
scorenames = results$classification.rankings[, 1]
norm_scores = data.frame(norm_score, scorenames, pbs_proportions)
norm_scores$majority_proportions = 1:nrow(norm_scores)
norm_scores[norm_scores$pbs_proportions > 0.5,]$majority_proportions = norm_scores[norm_scores$pbs_proportions > 0.5,]$pbs_proportions
norm_scores[norm_scores$pbs_proportions < 0.5,]$majority_proportions = 1 - norm_scores[norm_scores$pbs_proportions < 0.5,]$pbs_proportions
norm_scores$pbs_scores = 1:nrow(norm_scores)
norm_scores[norm_scores$scorenames == "pbs",]$pbs_scores = norm_scores[norm_scores$scorenames == "pbs",]$norm_score
norm_scores[norm_scores$scorenames == "mws",]$pbs_scores = 1 - norm_scores[norm_scores$scorenames == "mws",]$norm_score
norm_scores$mws_scores = 1 - norm_scores$pbs_scores
# Correlation between Normalized PBS scores & Proportion of PBS hand
cor(norm_scores$pbs_scores, pbs_proportions)
plot(norm_scores$pbs_scores, col = "grey", ylim = c(0,1))
lines(x = c(0, length(norm_scores$pbs_proportions)), y = c(0.5, 0.5), col = "red", lty = 2)
lines(norm_scores$pbs_proportions, col = "black")
# Correlation between Majority scores & Majority Proportion
cor(norm_scores$norm_score, norm_scores$majority_proportions)
summary(svm_classification)
summary(hand_majorities)
confusion_matrix = as.matrix(table(Handwriting = hand_majorities, Predicted = svm_classification))
confusion_matrix
num_classes = nrow(confusion_matrix)
diag = diag(confusion_matrix)
rowsums = apply(confusion_matrix, 1, sum)
colsums = apply(confusion_matrix, 2, sum)
p = rowsums / num_samples
q = colsums / num_samples
accuracy = sum(diag) / num_samples
accuracy
proportion_pbs = colsums["pbs"] / num_samples
proportion_pbs
precision = diag / colsums
recall = diag / rowsums
f1 = 2 * precision * recall / (precision + recall)
data.frame(precision, recall, f1)
macroPrecision = mean(precision)
macroRecall = mean(recall)
macroF1 = mean(f1)
data.frame(macroPrecision, macroRecall, macroF1)
### baseline for comparison ###
baseline_classification = rep("mws", num_samples)
confusion_matrix = as.matrix(table(Handwriting = hand_majorities, Predicted = baseline_classification))
confusion_matrix = cbind(confusion_matrix, pbs = c(0, 0))
confusion_matrix
num_classes = nrow(confusion_matrix)
diag = diag(confusion_matrix)
rowsums = apply(confusion_matrix, 1, sum)
colsums = apply(confusion_matrix, 2, sum)
p = rowsums / num_samples
q = colsums / num_samples
accuracy = sum(diag) / num_samples
accuracy
precision = c(diag["mws"] / colsums["mws"], 1)
names(precision) = c("mws", "pbs")
recall = diag / rowsums
f1 = 2 * precision * recall / (precision + recall)
data.frame(precision, recall, f1)
macroPrecision = mean(precision)
macroRecall = mean(recall)
macroF1 = mean(f1)
data.frame(macroPrecision, macroRecall, macroF1)
## some descriptive analysis
hand_df = data.frame(text_tokens, hand_tokens)
pbs_freqs = as.data.frame(table(hand_df[hand_df$hand_tokens == "pbs",]$text_tokens))
pbs_freqs = pbs_freqs[order(-pbs_freqs$Freq),]
pbs_freqs$relFreq = pbs_freqs$Freq / table(hand_tokens)["pbs"]
mws_freqs = as.data.frame(table(hand_df[hand_df$hand_tokens == "mws",]$text_tokens))
mws_freqs = mws_freqs[order(-mws_freqs$Freq),]
mws_freqs$relFreq = mws_freqs$Freq / table(hand_tokens)["mws"]
str(hand_majorities)
hand_majorities[213]
hand_majorities[214]
hand_majorities[200:220]
View(norm_scores)
library(stylo)
library(rjson)
setwd("/Users/tim/GitHub/frankenstein-v2/analysis")
#raw.corpus <- load.corpus(files = "all", corpus.dir = "./pca_texts",
#                          encoding = "UTF-8")
#tokenized.corpus <- txt.to.words.ext(raw.corpus, language = "English.all",
#                                     splitting.rule = "[!(;'?\n^).,>\":= \u2014\u2013]+")
glenarvon = fromJSON(file = "./tokenized_texts/lam_glenarvon.json")
thelastman = fromJSON(file = "./tokenized_texts/mws_the-last-man.json")
stirvyne = fromJSON(file = "./tokenized_texts/pbs_st-irvyne.json")
zastrozzi = fromJSON(file = "./tokenized_texts/pbs_zastrozzi.json")
frankenstein = fromJSON(file = "/Users/tim/GitHub/frankenstein-v2/sga-data/output/text_list_processed_ands.json")
tokenized.corpus = list(glenarvon, thelastman, stirvyne, zastrozzi, frankenstein)
names(tokenized.corpus) = c("lam_glenarvon", "mws_the-last-man", "pbs_st-irvyne", "pbs_zastrozzi", "???_frankenstein")
summary(tokenized.corpus)
sample_size = 800
sample_shift = 100
sample_overlap = sample_size - sample_shift
sliced.corpus <- make.samples(tokenized.corpus, sampling = "normal.sampling",
sample.overlap = sample_overlap,
sample.size = sample_size)
# Temporary list of frequent function words, eventually needs to based on Frankenstein as well
frequent.features <- fromJSON(file = "./f_words.json")
freqs <- make.table.of.frequencies(sliced.corpus, features = frequent.features)
#pca.results = stylo(frequencies = freqs, analysis.type = "PCR",
#      custom.graph.title = "Lamb vs. the Shelleys",
#      mfw.min = 200, mfw.max = 200,
#      pca.visual.flavour = "loadings",
#      write.png.file = FALSE, gui = FALSE)
#summary(pca.results)
#par(mfrow = c(1, 1))
# check how pca.coordinates are calculated from loadings and frequencies
#freq_table = pca.results$table.with.all.freqs
#load_table = t(pca.results$pca.rotation)
#coor_table = pca.results$pca.coordinates
#sum(freq_table["PBS_zastrozzi.txt_4",] * load_table["PC1",])
# fuck it let's do our own PCA
library(ggbiplot)
samples = as.data.frame(as.matrix.data.frame(freqs))
rnames = rownames(freqs)
cnames = colnames(freqs)
rownames(samples) = rnames
colnames(samples) = cnames
num_franken_samples = (length(frankenstein) - sample_overlap) %/% sample_shift
num_lam_samples = (length(glenarvon) - sample_overlap) %/% sample_shift
num_mws_samples = (length(thelastman) - sample_overlap) %/% sample_shift
num_pbs_samples = (length(stirvyne) - sample_overlap) %/% sample_shift + (length(zastrozzi) - sample_overlap) %/% sample_shift
training_samples = samples[1:(NROW(samples) - num_franken_samples),]
franken_samples = samples[(NROW(training_samples) + 1):NROW(samples),]
authors = c(rep("Lamb", num_lam_samples),
rep("Mary Shelley", num_mws_samples),
rep("Percy Shelley", num_pbs_samples))
training_samples.pca = prcomp(training_samples, center = TRUE, scale. = TRUE)
ggbiplot(training_samples.pca, labels = rownames(training_samples),
groups = authors, var.axes = TRUE, var.scale = 0.2, varname.adjust = 10)
frankenstein_sc = scale(franken_samples, center = training_samples.pca$center)
frankenstein_pred = frankenstein_sc %*% training_samples.pca$rotation
training_plusproj.pca = training_samples.pca
training_plusproj.pca$x = rbind(training_plusproj.pca$x, frankenstein_pred)
ggbiplot(training_plusproj.pca,
#         labels = rownames(rbind(training_samples, franken_samples)),
groups = c(authors, rep("Unknown", num_franken_samples)),
var.axes = TRUE, var.scale = 0.2, varname.adjust = 10)
# Add analysis in which PCs are trained on just the Shelley's
frequent.features <- fromJSON(file = "./f_words_shelleys.json")
freqs <- make.table.of.frequencies(sliced.corpus, features = frequent.features)
shelley_samples = training_samples[(num_lam_samples + 1):NROW(training_samples),]
shelley_samples.pca = prcomp(shelley_samples, center = TRUE, scale. = TRUE)
frankenstein_sc = scale(franken_samples, center = shelley_samples.pca$center)
frankenstein_pred = frankenstein_sc %*% shelley_samples.pca$rotation
shelley_plusproj.pca = shelley_samples.pca
shelley_plusproj.pca$x = rbind(shelley_plusproj.pca$x, frankenstein_pred)
ggbiplot(shelley_plusproj.pca,
#         labels = rownames(rbind(shelley_samples, franken_samples)),
groups = c(authors[(num_lam_samples + 1):NROW(training_samples)], rep("Unknown", num_franken_samples)),
var.axes = TRUE, var.scale = 0.5, varname.adjust = 10)
ggbiplot(shelley_plusproj.pca,
#         labels = rownames(rbind(shelley_samples, franken_samples)),
choices = c(1,3),
groups = c(authors[(num_lam_samples + 1):NROW(training_samples)], rep("Unknown", num_franken_samples)),
var.axes = TRUE, var.scale = 0.5, varname.adjust = 10)
num_franken_samples
str(frankenstein_pred)
type(frankenstein_pred)
class(frankenstein_pred)
class(franken_samples)
subset_start = 600
subset_end = 617
franken_pred_subset = frankenstein_pred[subset_start:subset_end,]
subset_length = NROW(franken_pred_subset)
shelley_plusproj.pca = shelley_samples.pca
shelley_plusproj.pca$x = rbind(shelley_plusproj.pca$x, franken_pred_subset)
ggbiplot(shelley_plusproj.pca,
#         labels = rownames(rbind(shelley_samples, franken_samples[subset_start:subset_end,])),
groups = c(authors[(num_lam_samples + 1):NROW(training_samples)], rep("Unknown", subset_length)),
var.axes = TRUE, var.scale = 0.5, varname.adjust = 10)
ggbiplot(shelley_plusproj.pca,
#         labels = rownames(rbind(shelley_samples, franken_samples[subset_start:subset_end,])),
choices = c(1,3),
groups = c(authors[(num_lam_samples + 1):NROW(training_samples)], rep("Unknown", subset_length)),
var.axes = TRUE, var.scale = 0.5, varname.adjust = 10)
ggbiplot(shelley_plusproj.pca,
#         labels = rownames(rbind(shelley_samples, franken_samples[subset_start:subset_end,])),
choices = c(1,3),
ellipse = TRUE,
groups = c(authors[(num_lam_samples + 1):NROW(training_samples)], rep("Unknown", subset_length)),
var.axes = TRUE, var.scale = 0.5, varname.adjust = 10)
ggbiplot(shelley_plusproj.pca,
#         labels = rownames(rbind(shelley_samples, franken_samples[subset_start:subset_end,])),
ellipse = TRUE,
groups = c(authors[(num_lam_samples + 1):NROW(training_samples)], rep("Unknown", subset_length)),
var.axes = TRUE, var.scale = 0.5, varname.adjust = 10)
ggbiplot(shelley_plusproj.pca,
#         labels = rownames(rbind(shelley_samples, franken_samples[subset_start:subset_end,])),
choices = c(1,3),
ellipse = TRUE,
groups = c(authors[(num_lam_samples + 1):NROW(training_samples)], rep("Unknown", subset_length)),
var.axes = TRUE, var.scale = 0.5, varname.adjust = 10)
ggbiplot(shelley_plusproj.pca,
#         labels = rownames(rbind(shelley_samples, franken_samples[subset_start:subset_end,])),
choices = c(1,4),
ellipse = TRUE,
groups = c(authors[(num_lam_samples + 1):NROW(training_samples)], rep("Unknown", subset_length)),
var.axes = TRUE, var.scale = 0.5, varname.adjust = 10)
ggbiplot(shelley_plusproj.pca,
#         labels = rownames(rbind(shelley_samples, franken_samples[subset_start:subset_end,])),
ellipse = TRUE,
groups = c(authors[(num_lam_samples + 1):NROW(training_samples)], rep("Unknown", subset_length)),
var.axes = TRUE, var.scale = 0.5, varname.adjust = 10)
ggbiplot(shelley_plusproj.pca,
#         labels = rownames(rbind(shelley_samples, franken_samples[subset_start:subset_end,])),
choices = c(3,4),
ellipse = TRUE,
groups = c(authors[(num_lam_samples + 1):NROW(training_samples)], rep("Unknown", subset_length)),
var.axes = TRUE, var.scale = 0.5, varname.adjust = 10)
ggbiplot(shelley_plusproj.pca,
#         labels = rownames(rbind(shelley_samples, franken_samples[subset_start:subset_end,])),
choices = c(2,4),
ellipse = TRUE,
groups = c(authors[(num_lam_samples + 1):NROW(training_samples)], rep("Unknown", subset_length)),
var.axes = TRUE, var.scale = 0.5, varname.adjust = 10)
ggbiplot(shelley_plusproj.pca,
#         labels = rownames(rbind(shelley_samples, franken_samples[subset_start:subset_end,])),
choices = c(2,3),
ellipse = TRUE,
groups = c(authors[(num_lam_samples + 1):NROW(training_samples)], rep("Unknown", subset_length)),
var.axes = TRUE, var.scale = 0.5, varname.adjust = 10)
ggbiplot(shelley_plusproj.pca,
#         labels = rownames(rbind(shelley_samples, franken_samples[subset_start:subset_end,])),
choices = c(1,3),
ellipse = TRUE,
groups = c(authors[(num_lam_samples + 1):NROW(training_samples)], rep("Unknown", subset_length)),
var.axes = TRUE, var.scale = 0.5, varname.adjust = 10)
library(stylo)
library(rjson)
library(zoo)
setwd("/Users/tim/GitHub/frankenstein-v2/analysis")
# tokenized.test.corpus = load.corpus.and.parse(corpus.dir = "./test_set",
#                                              encoding = "UTF-8",
#                                              splitting.rule = "[!(;'?\n^).,>\":= \u2014\u2013]+")
#summary(tokenized.test.corpus)
# write(toJSON(tokenized.test.corpus$S_text.txt), "frankenstein_tokenized_r.json")
text_tokens = fromJSON(file = "/Users/tim/GitHub/frankenstein-v2/sga-data/output/text_list_processed_ands.json")
tokenized.test.corpus = list(text_tokens)
names(tokenized.test.corpus) = c("S_text")
length(tokenized.test.corpus$S_text) == length(text_tokens)
# tokenized.training.corpus = load.corpus.and.parse(corpus.dir = "./reference_set",
#                                                  encoding = "UTF-8")
thelastman = fromJSON(file = "./tokenized_texts/mws_the-last-man.json")
stirvyne = fromJSON(file = "./tokenized_texts/pbs_st-irvyne.json")
zastrozzi = fromJSON(file = "./tokenized_texts/pbs_zastrozzi.json")
tokenized.training.corpus = list(thelastman, stirvyne, zastrozzi)
names(tokenized.training.corpus) = c("mws_the-last-man", "pbs_st-irvyne", "pbs_zastrozzi")
summary(tokenized.training.corpus)
sample_size = 100
sample_overlap = 0
resolution = sample_size - sample_overlap
sliced.test.corpus = make.samples(tokenized.test.corpus, sampling = "normal.sampling",
sample.size = sample_size, sample.overlap = sample_overlap)
sliced.training.corpus = make.samples(tokenized.training.corpus, sampling = "normal.sampling",
sample.size = sample_size, sample.overlap = 0)
function_words = fromJSON(file = "./f_words_shelleys.json")
results = rolling.classify(test.corpus = sliced.test.corpus,
training.corpus = sliced.training.corpus,
write.png.file = FALSE,
classification.method = "svm", features = function_words,
milestone.points = c(60214), milestone.labels = c("Fair Copy"),
slice.size = sample_size, slice.overlap = sample_overlap)
svm_classification = as.vector(results$classification.results)
hand_tokens = fromJSON(file = "/Users/tim/GitHub/frankenstein-v2/sga-data/output/hand_list_processed.json")
num_samples = as.integer((length(hand_tokens) - sample_overlap) / resolution)
num_words = (num_samples - 1) * resolution + sample_size
culled_hand_tokens = hand_tokens[1:num_words]
hand_matrix = rollapply(culled_hand_tokens, sample_size, by = resolution, c)
hand_groups = split(hand_matrix, row(hand_matrix))
hand_majorities = sapply(hand_groups, function(x) names(which.max(table(x))))
pbs_proportions = sapply(hand_groups, function(x) sum(x == "pbs") / sample_size)
norm_score = 0.5 + results$classification.scores[, 1] / (2 * max(results$classification.scores))
scorenames = results$classification.rankings[, 1]
norm_scores = data.frame(norm_score, scorenames, pbs_proportions)
norm_scores$majority_proportions = 1:nrow(norm_scores)
norm_scores[norm_scores$pbs_proportions > 0.5,]$majority_proportions = norm_scores[norm_scores$pbs_proportions > 0.5,]$pbs_proportions
norm_scores[norm_scores$pbs_proportions < 0.5,]$majority_proportions = 1 - norm_scores[norm_scores$pbs_proportions < 0.5,]$pbs_proportions
norm_scores$pbs_scores = 1:nrow(norm_scores)
norm_scores[norm_scores$scorenames == "pbs",]$pbs_scores = norm_scores[norm_scores$scorenames == "pbs",]$norm_score
norm_scores[norm_scores$scorenames == "mws",]$pbs_scores = 1 - norm_scores[norm_scores$scorenames == "mws",]$norm_score
norm_scores$mws_scores = 1 - norm_scores$pbs_scores
# Correlation between Normalized PBS scores & Proportion of PBS hand
cor(norm_scores$pbs_scores, pbs_proportions)
help("plot")
View(norm_scores)
plot(norm_scores$pbs_proportions * 100, ylim = c(0,1), type = "l")
plot(norm_scores$pbs_proportions * 100, ylim = c(0,100), type = "l")
plot(norm_scores$pbs_proportions * 100, ylim = c(0,100), type = "l", ylab = "Number of Words in Percy's hand")
plot(norm_scores$pbs_proportions * 100, ylim = c(0,100), type = "l", ylab = "% of Words in Percy's hand", xlab = "Sample Index (Sample size = 100 words)")
plot(norm_scores$pbs_proportions * 100, ylim = c(0,100), type = "l", ylab = "% of Words in Percy's hand", xlab = "Sample Index (Sample size = 100 words)", main = "Percy's contribution throughout Frankenstein")
plot(norm_scores$pbs_proportions * 100, ylim = c(0,100), type = "l", ylab = "% of words in Percy's hand", xlab = "Sample index (Sample size = 100 words)", main = "Percy's contribution throughout Frankenstein")
