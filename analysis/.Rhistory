fill=terrain.colors(3))
dev.off()
par()
#setEPS()
png("/Users/tim/Downloads/2_figuren_eps/whatever.png", width = 200, height = 100, res = 300)
par(mar=c(4, 4, 2, 4), xpd=TRUE, pin = c(5, 5), family="Helvetica")
barplot(md, ylab="Presence of phoneme (%)", beside=TRUE,
col=terrain.colors(3))
legend(15, 60, c("Obstruent","Liquid","Schwa"), cex=0.6,
fill=terrain.colors(3))
dev.off()
mydata <- data.frame(ple=c(76.36,50.91,12.73), tre=c(80.12, 57.76, 17.39),
kle=c(84.85, 59.09, 31.82), fre=c(100,28.57, 28.57))
md = as.matrix(mydata)
colnames(md)= c("/plǝ/", "/tʁǝ/", "/klǝ/", "/fʁǝ/")
ok_pars = par(mar=c(4, 4, 2, 4), xpd=TRUE, family="Times New Roman")
#setEPS()
png("/Users/tim/Downloads/2_figuren_eps/whatever.png", width = 200, height = 100, res = 300)
par(mar=c(4, 4, 2, 4), xpd=TRUE, pin = c(4, 4), family="Helvetica")
barplot(md, ylab="Presence of phoneme (%)", beside=TRUE,
col=terrain.colors(3))
legend(15, 60, c("Obstruent","Liquid","Schwa"), cex=0.6,
fill=terrain.colors(3))
dev.off()
mydata <- data.frame(ple=c(76.36,50.91,12.73), tre=c(80.12, 57.76, 17.39),
kle=c(84.85, 59.09, 31.82), fre=c(100,28.57, 28.57))
md = as.matrix(mydata)
colnames(md)= c("/plǝ/", "/tʁǝ/", "/klǝ/", "/fʁǝ/")
ok_pars = par(mar=c(4, 4, 2, 4), xpd=TRUE, family="Times New Roman")
#setEPS()
png("/Users/tim/Downloads/2_figuren_eps/whatever.png", width = 200, height = 100, res = 300)
par(mar=c(4, 4, 2, 4), xpd=TRUE, pin = c(4, 2), family="Helvetica")
barplot(md, ylab="Presence of phoneme (%)", beside=TRUE,
col=terrain.colors(3))
legend(15, 60, c("Obstruent","Liquid","Schwa"), cex=0.6,
fill=terrain.colors(3))
dev.off()
par(mar=c(4, 4, 2, 4), xpd=TRUE, pin = c(2, 1), family="Helvetica")
#setEPS()
png("/Users/tim/Downloads/2_figuren_eps/whatever.png", width = 200, height = 100, res = 300)
par(mar=c(4, 4, 2, 4), xpd=TRUE, pin = c(2, 1), family="Helvetica")
barplot(md, ylab="Presence of phoneme (%)", beside=TRUE,
col=terrain.colors(3))
legend(15, 60, c("Obstruent","Liquid","Schwa"), cex=0.6,
fill=terrain.colors(3))
dev.off()
#setEPS()
png("/Users/tim/Downloads/2_figuren_eps/whatever.png", width = 200, height = 100, res = 300)
par(mar=c(4, 4, 2, 4), xpd=TRUE, pin = c(1, 1), family="Helvetica")
barplot(md, ylab="Presence of phoneme (%)", beside=TRUE,
col=terrain.colors(3))
legend(15, 60, c("Obstruent","Liquid","Schwa"), cex=0.6,
fill=terrain.colors(3))
dev.off()
#setEPS()
png("/Users/tim/Downloads/2_figuren_eps/whatever.png", width = 200, height = 100, res = 300)
par(mar=c(4, 4, 2, 4), xpd=TRUE, pin = c(0.5, 0.5), family="Helvetica")
barplot(md, ylab="Presence of phoneme (%)", beside=TRUE,
col=terrain.colors(3))
legend(15, 60, c("Obstruent","Liquid","Schwa"), cex=0.6,
fill=terrain.colors(3))
dev.off()
#setEPS()
png("/Users/tim/Downloads/2_figuren_eps/whatever.png", width = 200, height = 100, res = 300)
par(mar=c(4, 4, 2, 4), xpd=TRUE, pin = c(0.5, 0.5), family="Helvetica")
barplot(md, ylab="Presence of phoneme (%)", beside=TRUE,
col=terrain.colors(3))
legend(15, 60, c("Obstruent","Liquid","Schwa"), cex=0.6,
fill=terrain.colors(3))
dev.off()
#setEPS()
png("/Users/tim/Downloads/2_figuren_eps/whatever.png", width = 200, height = 100, res = 300)
par(mar=c(4, 4, 2, 4), xpd=TRUE, pin = c(0.5, 0.5), family="Helvetica")
barplot(md, ylab="Presence of phoneme (%)", beside=TRUE,
col=terrain.colors(3))
legend(15, 60, c("Obstruent","Liquid","Schwa"), cex=0.6,
fill=terrain.colors(3))
dev.off()
#setEPS()
png("/Users/tim/Downloads/2_figuren_eps/whatever.png", width = 200, height = 100, res = 300)
par(mar=c(4, 4, 2, 4), xpd=TRUE, pin = c(2, 2), family="Helvetica")
barplot(md, ylab="Presence of phoneme (%)", beside=TRUE,
col=terrain.colors(3))
legend(15, 60, c("Obstruent","Liquid","Schwa"), cex=0.6,
fill=terrain.colors(3))
dev.off()
FTO_data <- read.csv("/Users/tim/Documents/IFADVcorpus/FTO_data_pos_processed.csv")
mean(FTO_data$floor_transfer_offset)
FTO_data <- read.csv("/Users/tim/Documents/IFADVcorpus/FTO_data_auto_pos_processed.csv")
mean(FTO_data$floor_transfer_offset)
library(languageR)
str("alice")
str(alice)
library(languageR)
library(jsonlite)
fileName = 'atlantic_tokens.json'
text = readChar(fileName, file.info(fileName)$size)
fileName = './atlantic_tokens.json'
text = readChar(fileName, file.info(fileName)$size)
fileName = '/atlantic_tokens.json'
text = readChar(fileName, file.info(fileName)$size)
fileName = 'atlantic_tokens.json'
text = readChar(fileName, file.info(fileName)$size)
library(languageR)
library(rjson)
text = fromJSON(file = "atlantic_tokens.json")
timing = read.csv("timing_data.csv")
scores = read.csv("/Users/tim/OneDrive/Master/Master_thesis/data/PitchDisc_dataset.csv")
View(scores)
str(scores)
scores$participant = factor(scores$participant)
scores$experiment = factor(scores$experiment)
scores$participant = factor(scores$participant)
scores$experiment = factor(scores$experiment)
m1 = lm(score ~ experiment, data = scores)
summary(m1)
library(lmerTest)
scores = read.csv("/Users/tim/OneDrive/Master/Master_thesis/data/PitchDisc_dataset.csv")
scores$participant = factor(scores$participant)
scores$experiment = factor(scores$experiment)
m1 = lmer(score ~ experiment + (1|participant), data = scores)
summary(m1)
scores = read.csv("/Users/tim/OneDrive/Master/Master_thesis/data/PitchDisc_dataset.csv")
View(scores)
scores$participant = factor(scores$participant)
scores$experiment = factor(scores$experiment)
m1 = lm(simple_score ~ experiment + (1|participant), data = scores)
summary(m1)
m1 = lm(weighted_score ~ experiment + (1|participant), data = scores)
summary(m1)
scores = read.csv("/Users/tim/OneDrive/Master/Master_thesis/data/PitchDisc_dataset.csv")
scores$participant = factor(scores$participant)
scores$experiment = factor(scores$experiment)
m1 = lm(weighted_score ~ experiment + (1|participant), data = scores)
summary(m1)
scores = read.csv("/Users/tim/OneDrive/Master/Master_thesis/data/PitchDisc_dataset.csv")
scores$participant = factor(scores$participant)
scores$experiment = factor(scores$experiment)
m1 = lm(weighted_score ~ experiment, data = scores)
summary(m1)
scores = read.csv("/Users/tim/OneDrive/Master/Master_thesis/data/PitchDisc_dataset.csv")
scores$participant = factor(scores$participant)
scores$experiment = factor(scores$experiment)
m1 = lm(simple_score ~ experiment, data = scores)
summary(m1)
eff = allEffects(m1)
library(effects)
eff = allEffects(m1)
plot(eff$experiment)
ggplot(scores, aes(x=simple_score)) + geom_density(aes(fill=experiment), alpha=0.4)
library(ggplot2)
ggplot(scores, aes(x=simple_score)) + geom_density(aes(fill=experiment), alpha=0.4)
ggplot(scores, aes(x=weighted_score)) + geom_density(aes(fill=experiment), alpha=0.4)
ggplot(scores, aes(x=log(weighted_score))) + geom_density(aes(fill=experiment), alpha=0.4)
ggplot(scores, aes(x=sqrt(weighted_score))) + geom_density(aes(fill=experiment), alpha=0.4)
ggplot(scores, aes(x=weighted_score)) + geom_density(aes(fill=experiment), alpha=0.4)
library(effects)
library(ggplot2)
scores = read.csv("/Users/tim/OneDrive/Master/Master_thesis/data/PitchDisc_dataset.csv")
scores$participant = factor(scores$participant)
scores$experiment = factor(scores$experiment)
m1 = lm(simple_score ~ experiment, data = scores)
summary(m1)
eff = allEffects(m1)
ggplot(scores, aes(x=weighted_score)) + geom_density(aes(fill=experiment), alpha=0.4)
scores = read.csv("/Users/tim/OneDrive/Master/Master_thesis/data/PitchDisc_dataset.csv")
scores$participant = factor(scores$participant)
scores$experiment = factor(scores$experiment)
m1 = lm(weighted_score ~ experiment, data = scores)
summary(m1)
eff = allEffects(m1)
plot(eff$experiment)
m1 = lm(simple_score ~ experiment, data = scores)
summary(m1)
eff = allEffects(m1)
plot(eff$experiment)
t.test(simple_score ~ experiment, data = scores)
library(lmerTest)
library(effects)
library(lattice)
library(lsmeans)
MMAT2 <- read.csv("MMAT2_dataset_v2.csv")
MMAT3 <- read.csv("MMAT3_dataset_v2.csv")
MMAT2$Experiment = "1"
MMAT3$Experiment = "2"
combined = rbind(MMAT2, MMAT3)
# Rename variables
combined$PitchDifference = factor(combined$PitchDifference)
levels(combined$PitchDifference) = c("Same", "Different")
combined$Experiment = factor(combined$Experiment)
combined$Room = factor(combined$Room)
names(combined)[names(combined) == "PitchDifference"] = "Pitch"
names(combined)[names(combined) == "log10_SUBTLEX"] = "WordFrequency"
names(combined)[names(combined) == "stim_dur"] = "WordLength"
names(combined)[names(combined) == "PrimeRT"] = "Prime_RT"
names(combined)[names(combined) == "LogPrimeRT"] = "PrimeRT"
names(combined)[names(combined) == "LogPrevRT"] = "PreviousRT"
# Remove irrelevant data and extreme outliers
restricted = combined[combined$Type == "word" & combined$Accuracy == "correct"
& combined$PrimeAccuracy == "correct" & combined$RT < 40000
& combined$LogRT > 1,]
# Inspect normality of participants
qqmath(~LogRT|Participant, data = restricted)
# Remove thick right tails (cf. Baayen 2008)
trimmed = restricted[restricted$LogRT < 3.2,]
pitchdata = read.csv("/Users/tim/GitHub/MMAT/stimuli_pitch.csv")
View(pitchdata)
pitchdata$original_pitch = pitchdata$hz / pitchdata$shift
mean(pitchdata[pitchdata$category == "1",]$original_pitch)
NROW(pitchdata[pitchdata$category == "1",]$original_pitch)
sd(pitchdata[pitchdata$category == "1",]$original_pitch)
M = 2.22
SD = 0.46
library(lattice)
library(grid)
library(gplots)
# library(lme4)
library(effects)
library(lmerTest)
library(ggplot2)
slider_data = read.csv("/Users/tim/OneDrive/Master/Master_thesis/slider/slider_data_ext.csv")
slider_data$participant = factor(slider_data$participant)
slider_data$pitch_difference = slider_data$erb_shift + 0.4
slider_data$pitch_difference_log = log(slider_data$pitch_difference)
slider_data$pitch_change = slider_data$erb_shift - slider_data$starting_erb
names(slider_data)[names(slider_data) == "starting_erb"] = "starting_pitch"
slider_data$starting_pitch_difference = slider_data$starting_pitch + 0.4
# Excluding pp 2 & 16
mypanelfn <- function(x,y,...){
panel.xyplot(x=x, y=y, ...)
grid.lines(c(0,1), unit(rep(1.2, 2), "native"), gp=gpar(col = "red"))}
xyplot(pitch_difference ~ starting_pitch_difference | participant, data=slider_data, xlab = "Starting Pitch Difference (ERB)", ylab = "Doubled Pitch Difference (ERB)", layout = c(10,2), panel = mypanelfn)
xyplot(pitch_change ~ starting_pitch_difference | participant, data=slider_data, xlab = "Starting Pitch Difference (ERB)", ylab = "Pitch Adjustment (ERB)", layout = c(10,2))
mean(slider_data$erb_shift)
cor(slider_data[slider_data$participant == "16",]$pitch_difference, slider_data[slider_data$participant == "16",]$starting_pitch_difference)
cor(slider_data[slider_data$participant == "5",]$pitch_change, slider_data[slider_data$participant == "5",]$starting_pitch_difference)
participants = slider_data[slider_data$trial == 6,]$participant
correlations = c()
for(i in participants){
correlation = cor(slider_data[slider_data$participant == i,]$pitch_change, slider_data[slider_data$participant == i,]$starting_pitch_difference)
correlations = c(correlations, correlation)
}
cor_df = data.frame(participants, correlations)
cor_df$trans_cor = 0.5 * log((1 + cor_df$correlations) / (1 - cor_df$correlations))
plot(sort(cor_df$correlations), main = "Correlations between Pitch Adjustment and Starting Pitch Difference", ylab = "Pearson's r", type = "n")
text(1:nrow(cor_df), sort(cor_df$correlations), as.character(cor_df[order(correlations),]$participants), cex = 0.8)
trans_qq = qqnorm(r2t(cor_df$correlations, 18), plot.it = FALSE)
plot(trans_qq$x,trans_qq$y, type = "n")
text(trans_qq$x, trans_qq$y, as.character(cor_df$participants), cex = 0.8)
qqline(r2t(cor_df$correlations, 18))
good_pp = slider_data[slider_data$participant != "2" & slider_data$participant != "16", ]
# model
m1 = lmer(pitch_difference ~ starting_pitch_difference + (1 + starting_pitch_difference|participant) + (1|stimulus), good_pp)
m2 = lmer(pitch_difference ~ starting_pitch_difference + music + (1 + starting_pitch_difference|participant) + (1|stimulus), good_pp)
m3 = lmer(pitch_difference ~ starting_pitch_difference + strategy + (1 + starting_pitch_difference|participant) + (1|stimulus), good_pp)
m4 = lmer(pitch_difference ~ starting_pitch_difference + trial + (1 + starting_pitch_difference|participant) + (1|stimulus), good_pp)
m5 = lmer(pitch_difference ~ starting_pitch_difference + num_adjustments + (1 + starting_pitch_difference|participant) + (1|stimulus), good_pp)
anova(m1, m2)
anova(m1, m3)
anova(m1, m4)
anova(m1, m5)
fitted_dataset = good_pp[abs(scale(resid(m1))) < 2.5,]
mean(fitted_dataset$pitch_difference)
m_fitted = lmer(pitch_difference ~ starting_pitch_difference + (1 + starting_pitch_difference|participant) + (1|stimulus), fitted_dataset)
summary(m_fitted)
par(mfrow=c(1,2))
qqnorm(residuals(m1), main = "Original Model")
qqline(residuals(m1), col = "red")
qqnorm(residuals(m_fitted), main = "Refitted Model")
qqline(residuals(m_fitted), col = "red")
par(mfrow=c(1,1))
t.test(fitted_dataset$pitch_difference, 2.4)
t.test(fitted_dataset$pitch_difference, mu=2.4)
t.test(log(fitted_dataset$pitch_difference), mu=log(2.4))
t.test(log(fitted_dataset$pitch_difference), mu=log(1.2))
t.test(fitted_dataset$pitch_difference, fitted_dataset$starting_pitch_difference)
mean(predicted(m1))
mean(predict(m1))
?predict
mean(predict(m_fitted))
sd(predict(m_fitted))
mean(log(fitted_dataset$pitch_difference))
exp(0.7753235)
shapiro.test(fitted_dataset$pitch_difference)
shapiro.test(log(fitted_dataset$pitch_difference))
cor(fitted_dataset$pitch_change, fitted_dataset$starting_pitch_difference)
cor.test(fitted_dataset$pitch_change, fitted_dataset$starting_pitch_difference)
library(stylo)
stylo(corpus.dir = "pca_texts", mfw.min = 200, mfw.max = 200,
analysis.type = "PCR", sampling = "normal.sampling", sample.size = 10000,
gui = FALSE)
library(stylo)
stylo(corpus.dir = "./pca_texts", mfw.min = 200, mfw.max = 200,
analysis.type = "PCR", sampling = "normal.sampling", sample.size = 10000,
gui = FALSE)
library(stylo)
stylo(corpus.dir = "/Users/tim/GitHub/frankenstein-v2/pca_texts", mfw.min = 200, mfw.max = 200,
analysis.type = "PCR", sampling = "normal.sampling", sample.size = 10000,
gui = FALSE)
library(stylo)
stylo(corpus.dir = "/Users/tim/GitHub/frankenstein-v2/pca_texts", mfw.min = 200, mfw.max = 200,
analysis.type = "PCR", sampling = "normal.sampling", sample.size = 10000, pca.visual.flavour = "loadings",
gui = FALSE)
raw.corpus <- load.corpus(files = "all", corpus.dir = "/Users/tim/GitHub/frankenstein-v2/pca_texts",
encoding = "UTF-8")
tokenized.corpus <- txt.to.words.ext(raw.corpus, language = "English.all",
preserve.case = FALSE)
summary(tokenized.corpus)
sliced.corpus <- make.samples(tokenized.corpus, sampling = "normal.sampling",
sample.size = 10000)
frequent.features <- make.frequency.list(sliced.corpus, head = 200)
frequent.features
str
str(frequent.features)
library(rjson)
library(stylo)
library(rjson)
raw.corpus <- load.corpus(files = "all", corpus.dir = "/Users/tim/GitHub/frankenstein-v2/pca_texts",
encoding = "UTF-8")
tokenized.corpus <- txt.to.words.ext(raw.corpus, language = "English.all",
preserve.case = FALSE)
summary(tokenized.corpus)
sliced.corpus <- make.samples(tokenized.corpus, sampling = "normal.sampling",
sample.size = 10000)
frequent.features <- fromJSON(file = "/Users/tim/GitHub/frankenstein-v2/f_words.json")
frequent.features
frequent.features <- make.frequency.list(sliced.corpus, head = 3000)
frequent.features
frequent.features <- make.frequency.list(sliced.corpus, head = 200)
frequent.features
library(stylo)
library(rjson)
raw.corpus <- load.corpus(files = "all", corpus.dir = "/Users/tim/GitHub/frankenstein-v2/pca_texts",
encoding = "UTF-8")
tokenized.corpus <- txt.to.words.ext(raw.corpus, language = "English.all",
preserve.case = FALSE)
summary(tokenized.corpus)
sliced.corpus <- make.samples(tokenized.corpus, sampling = "normal.sampling",
sample.size = 10000)
# Temporary list of frequent function words, eventually needs to based on Frankenstein as well
frequent.features <- fromJSON(file = "/Users/tim/GitHub/frankenstein-v2/f_words.json")
freqs <- make.table.of.frequencies(sliced.corpus, features = frequent.features)
stylo(frequencies = freqs, analysis.type = "PCR",
custom.graph.title = "Lamb vs. the Shelleys",
pca.visual.flavour = "loadings",
write.png.file = FALSE, gui = FALSE)
stylo(frequencies = freqs, analysis.type = "PCR",
custom.graph.title = "Lamb vs. the Shelleys",
pca.visual.flavour = "technical",
write.png.file = FALSE, gui = FALSE)
library(stylo)
library(rjson)
raw.corpus <- load.corpus(files = "all", corpus.dir = "/Users/tim/GitHub/frankenstein-v2/pca_texts",
encoding = "UTF-8")
tokenized.corpus <- txt.to.words.ext(raw.corpus, language = "English.all",
preserve.case = FALSE)
summary(tokenized.corpus)
sliced.corpus <- make.samples(tokenized.corpus, sampling = "normal.sampling",
sample.size = 10000)
# Temporary list of frequent function words, eventually needs to based on Frankenstein as well
frequent.features <- fromJSON(file = "/Users/tim/GitHub/frankenstein-v2/f_words.json")
freqs <- make.table.of.frequencies(sliced.corpus, features = frequent.features)
stylo(frequencies = freqs, analysis.type = "PCR",
custom.graph.title = "Lamb vs. the Shelleys",
pca.visual.flavour = "technical",
write.png.file = FALSE, gui = FALSE)
text_tokens = fromJSON(file = "/Users/tim/GitHub/frankenstein-v2/sga-data/output/text_list_processed.json")
library(rjson)
text_tokens = fromJSON(file = "/Users/tim/GitHub/frankenstein-v2/sga-data/output/text_list_processed.json")
hand_tokens = fromJSON(file = "/Users/tim/GitHub/frankenstein-v2/sga-data/output/hand_list_processed.json")
?data.frame
hand_df = data.frame(text_tokens, hand_tokens)
View(hand_df)
pbs_freqs = table(hand_df[hand_df$hand_tokens == "pbs",])
pbs_freqs
pbs_freqs = table(hand_df[,hand_df$hand_tokens == "pbs"])
length(hand_df[hand_df$hand_tokens == "pbs",])
length(hand_df[hand_df$hand_tokens == "pbs",]$text_tokens)
length(hand_df$text_tokens)
pbs_freqs = table(hand_df[,hand_df$hand_tokens == "pbs"]$text_tokens)
pbs_freqs = table(hand_df[hand_df$hand_tokens == "pbs",]$text_tokens)
pbs_freqs
length(hand_df[hand_df$hand_tokens == "pbs",]$text_tokens)
length(hand_df$text_tokens)
pbs_df = hand_df[hand_df$hand_tokens == "pbs",]
View(pbs_df)
table(pbs_df$text_tokens)
pbs_freqs = as.data.frame(table(hand_df[hand_df$hand_tokens == "pbs",]$text_tokens))
View(pbs_freqs)
pbs_freqs = pbs_freqs[order(Freq),]
pbs_freqs = pbs_freqs[order(pbs_freqs$Freq),]
pbs_freqs = pbs_freqs[order(-pbs_freqs$Freq),]
mws_freqs = as.data.frame(table(hand_df[hand_df$hand_tokens == "mws",]$text_tokens))
mws_freqs = pbs_freqs[order(-pbs_freqs$Freq),]
View(mws_freqs)
mws_freqs = mws_freqs[order(-mws_freqs$Freq),]
View(mws_freqs)
hand_df = data.frame(text_tokens, hand_tokens)
pbs_freqs = as.data.frame(table(hand_df[hand_df$hand_tokens == "pbs",]$text_tokens))
pbs_freqs = pbs_freqs[order(-pbs_freqs$Freq),]
mws_freqs = as.data.frame(table(hand_df[hand_df$hand_tokens == "mws",]$text_tokens))
mws_freqs = mws_freqs[order(-mws_freqs$Freq),]
text_tokens = fromJSON(file = "/Users/tim/GitHub/frankenstein-v2/sga-data/output/text_list_processed.json")
hand_tokens = fromJSON(file = "/Users/tim/GitHub/frankenstein-v2/sga-data/output/hand_list_processed.json")
hand_df = data.frame(text_tokens, hand_tokens)
pbs_freqs = as.data.frame(table(hand_df[hand_df$hand_tokens == "pbs",]$text_tokens))
pbs_freqs = pbs_freqs[order(-pbs_freqs$Freq),]
mws_freqs = as.data.frame(table(hand_df[hand_df$hand_tokens == "mws",]$text_tokens))
mws_freqs = mws_freqs[order(-mws_freqs$Freq),]
View(mws_freqs)
View(pbs_freqs)
hand_tokens["pbs"]
table(hand_tokens)
table(hand_tokens)["pbs"]
table(hand_tokens)["pbs"] + 2
table(hand_tokens)["pbs"]
7172 / table(hand_tokens)["pbs"]
pbs_freqs$relFreq = pbs_freqs$Freq / table(hand_tokens)["pbs"]
mws_freqs$relFreq = mws_freqs$Freq / table(hand_tokens)["mws"]
View(mws_freqs)
View(pbs_freqs)
help("rollapply")
library(zoo)
help("rollapply")
hand_tokens = fromJSON(file = "/Users/tim/GitHub/frankenstein-v2/sga-data/output/hand_list_processed.json")
library(rjson)
text_tokens = fromJSON(file = "/Users/tim/GitHub/frankenstein-v2/sga-data/output/text_list_processed_ands.json")
hand_tokens = fromJSON(file = "/Users/tim/GitHub/frankenstein-v2/sga-data/output/hand_list_processed.json")
hand_df = data.frame(text_tokens, hand_tokens)
pbs_freqs = as.data.frame(table(hand_df[hand_df$hand_tokens == "pbs",]$text_tokens))
pbs_freqs = pbs_freqs[order(-pbs_freqs$Freq),]
pbs_freqs$relFreq = pbs_freqs$Freq / table(hand_tokens)["pbs"]
mws_freqs = as.data.frame(table(hand_df[hand_df$hand_tokens == "mws",]$text_tokens))
mws_freqs = mws_freqs[order(-mws_freqs$Freq),]
mws_freqs$relFreq = mws_freqs$Freq / table(hand_tokens)["mws"]
View(pbs_freqs)
head(pbs_freqs, 5)
head(as.matrix(pbs_freqs), 5)
View(hand_df)
library(stylo)
library(rjson)
setwd("/Users/tim/GitHub/frankenstein-v2/analysis")
#raw.corpus <- load.corpus(files = "all", corpus.dir = "./pca_texts",
#                          encoding = "UTF-8")
#tokenized.corpus <- txt.to.words.ext(raw.corpus, language = "English.all",
#                                     splitting.rule = "[!(;'?\n^).,>\":= \u2014\u2013]+")
glenarvon = fromJSON(file = "./tokenized_texts/lam_glenarvon.json")
thelastman = fromJSON(file = "./tokenized_texts/mws_the-last-man.json")
stirvyne = fromJSON(file = "./tokenized_texts/pbs_st-irvyne.json")
zastrozzi = fromJSON(file = "./tokenized_texts/pbs_zastrozzi.json")
frankenstein = fromJSON(file = "/Users/tim/GitHub/frankenstein-v2/sga-data/output/text_list_processed_ands.json")
tokenized.corpus = list(glenarvon, thelastman, stirvyne, zastrozzi, frankenstein)
names(tokenized.corpus) = c("lam_glenarvon", "mws_the-last-man", "pbs_st-irvyne", "pbs_zastrozzi", "???_frankenstein")
summary(tokenized.corpus)
sample_size = 800
sample_shift = 100
sample_overlap = sample_size - sample_shift
sliced.corpus <- make.samples(tokenized.corpus, sampling = "normal.sampling",
sample.overlap = sample_overlap,
sample.size = sample_size)
str(sliced.corpus)
frequent.features <- fromJSON(file = "./f_words.json")
freqs <- make.table.of.frequencies(sliced.corpus, features = frequent.features)
str(freqs)
sample_size = 800
sample_overlap = 700
resolution = sample_size - sample_overlap
library(rjson)
library(zoo)
hand_tokens = fromJSON(file = "/Users/tim/GitHub/frankenstein-v2/sga-data/output/hand_list_processed.json")
num_samples = as.integer((length(hand_tokens) - sample_overlap) / resolution)
num_words = (num_samples - 1) * resolution + sample_size
culled_hand_tokens = hand_tokens[1:num_words]
hand_matrix = rollapply(culled_hand_tokens, sample_size, by = resolution, c)
hand_groups = split(hand_matrix, row(hand_matrix))
hand_majorities = sapply(hand_groups, function(x) names(which.max(table(x))))
hand_majorities
View(hand_groups)
class(hand_majorities)
View(sliced.corpus)
help("paste")
library(stylo)
library(rjson)
setwd("/Users/tim/GitHub/frankenstein-v2/analysis")
#raw.corpus <- load.corpus(files = "all", corpus.dir = "./pca_texts",
#                          encoding = "UTF-8")
#tokenized.corpus <- txt.to.words.ext(raw.corpus, language = "English.all",
#                                     splitting.rule = "[!(;'?\n^).,>\":= \u2014\u2013]+")
glenarvon = fromJSON(file = "./tokenized_texts/lam_glenarvon.json")
thelastman = fromJSON(file = "./tokenized_texts/mws_the-last-man.json")
stirvyne = fromJSON(file = "./tokenized_texts/pbs_st-irvyne.json")
zastrozzi = fromJSON(file = "./tokenized_texts/pbs_zastrozzi.json")
frankenstein = fromJSON(file = "/Users/tim/GitHub/frankenstein-v2/sga-data/output/text_list_processed_ands.json")
tokenized.corpus = list(glenarvon, thelastman, stirvyne, zastrozzi, frankenstein)
names(tokenized.corpus) = c("lam_glenarvon", "mws_the-last-man", "pbs_st-irvyne", "pbs_zastrozzi", "???_frankenstein")
summary(tokenized.corpus)
sample_size = 800
sample_shift = 100
sample_overlap = sample_size - sample_shift
sliced.corpus <- make.samples(tokenized.corpus, sampling = "normal.sampling",
sample.overlap = sample_overlap,
sample.size = sample_size)
# Temporary list of frequent function words, eventually needs to based on Frankenstein as well
frequent.features <- fromJSON(file = "./f_words.json")
freqs <- make.table.of.frequencies(sliced.corpus, features = frequent.features)
pca.results = stylo(frequencies = freqs, analysis.type = "PCR",
custom.graph.title = "Lamb vs. the Shelleys",
mfw.min = 200, mfw.max = 200,
pca.visual.flavour = "loadings",
write.png.file = FALSE, gui = FALSE)
samples = as.data.frame(as.matrix.data.frame(freqs))
View(samples)
